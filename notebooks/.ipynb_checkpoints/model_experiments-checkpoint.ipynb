{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded996b1",
   "metadata": {},
   "source": [
    "# ISL Sign Recognition – Experiment Dossier\n",
    "\n",
    "This notebook is crafted for presenting the Indian Sign Language recognition system to judges. It summarises the dataset, documents every conducted experiment, and highlights the best-performing checkpoint with reproducible configuration details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02351367",
   "metadata": {},
   "source": [
    "## Quick start checklist\n",
    "\n",
    "1. Activate the project environment: `!pyenv activate miniGes`\n",
    "2. Confirm you are in the repository root (`sinGes(mini)/`).\n",
    "3. Ensure the INCLUDE dataset is available under `dataset/` (see `Roadmap.md`).\n",
    "4. Run each experiment cell sequentially; results are captured in the summary table automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d493df",
   "metadata": {},
   "source": [
    "## Project overview\n",
    "\n",
    "- **Objective:** 54-class Indian Sign Language recognition powered by a pretrained 3D ResNet (R3D-18) backbone.\n",
    "- **Baseline accuracy:** ~35% Top-1 / ~65% Top-5 on validation split.\n",
    "- **Success criteria:** Push Top-1 accuracy beyond the current baseline while maintaining robust Top-5 coverage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff215ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmp.dataset_summary import gather_counts\n",
    "import pandas as pd\n",
    "\n",
    "counts = gather_counts(PROJECT_ROOT / 'dataset')\n",
    "df_counts = (\n",
    "    pd.DataFrame(sorted(counts.items()), columns=['label', 'videos'])\n",
    "    .sort_values('videos', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "display(df_counts.head(10))\n",
    "display(df_counts.tail(10))\n",
    "print(f\"Total classes: {df_counts.shape[0]} | Total videos: {int(df_counts['videos'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a921de",
   "metadata": {},
   "source": [
    "## Experiment registry\n",
    "\n",
    "Each experiment is defined by a name, configuration overrides, and the resulting metrics. The registry below collects results so judges can quickly compare experiments and identify the best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05759484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "TRAIN_CONFIG_PATH = PROJECT_ROOT / 'config' / 'config.yaml'\n",
    "MODEL_CONFIG_PATH = PROJECT_ROOT / 'config' / 'model_config.yaml'\n",
    "\n",
    "\n",
    "def load_configs():\n",
    "    with TRAIN_CONFIG_PATH.open() as fp:\n",
    "        train_cfg = yaml.safe_load(fp)\n",
    "    with MODEL_CONFIG_PATH.open() as fp:\n",
    "        model_cfg = yaml.safe_load(fp)\n",
    "    return train_cfg, model_cfg\n",
    "\n",
    "\n",
    "train_cfg_base, model_cfg_base = load_configs()\n",
    "\n",
    "print('Recognition training config:')\n",
    "pprint(train_cfg_base['training']['recognition'])\n",
    "print('\n",
    "Model config:')\n",
    "pprint(model_cfg_base['sign_recognition'])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    name: str\n",
    "    training_overrides: Dict[str, Any]\n",
    "    model_overrides: Dict[str, Any]\n",
    "    notes: str\n",
    "    result: Dict[str, Any] | None = None\n",
    "\n",
    "\n",
    "EXPERIMENTS: List[Experiment] = [\n",
    "    Experiment(\n",
    "        name=\"Baseline – Pretrained R3D-18\",\n",
    "        training_overrides={},\n",
    "        model_overrides={},\n",
    "        notes=\"Reference run using config defaults (layer4 unfrozen, 25 epochs).\",\n",
    "    ),\n",
    "    Experiment(\n",
    "        name=\"FT Layers 3&4 + Longer Training\",\n",
    "        training_overrides={\n",
    "            'learning_rate': 2e-4,\n",
    "            'batch_size': 4,\n",
    "            'num_epochs': 30,\n",
    "            'early_stopping_patience': 10,\n",
    "        },\n",
    "        model_overrides={\n",
    "            'freeze_backbone': False,\n",
    "            'unfreeze_layers': ['layer3', 'layer4'],\n",
    "        },\n",
    "        notes=\"Fine-tune deeper layers with extended training and lower LR.\",\n",
    "    ),\n",
    "    Experiment(\n",
    "        name=\"Label Smoothing + Patience\",\n",
    "        training_overrides={\n",
    "            'learning_rate': 2.5e-4,\n",
    "            'num_epochs': 30,\n",
    "            'early_stopping_patience': 12,\n",
    "        },\n",
    "        model_overrides={\n",
    "            'freeze_backbone': False,\n",
    "            'unfreeze_layers': ['layer4'],\n",
    "        },\n",
    "        notes=\"Placeholder for label smoothing variant (implemented in code when ready).\",\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba588144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def _deep_update(mapping, updates):\n",
    "    for key, value in updates.items():\n",
    "        if isinstance(value, dict) and isinstance(mapping.get(key), dict):\n",
    "            _deep_update(mapping[key], value)\n",
    "        else:\n",
    "            mapping[key] = value\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def apply_experiment_overrides(training_overrides=None, model_overrides=None):\n",
    "    training_overrides = training_overrides or {}\n",
    "    model_overrides = model_overrides or {}\n",
    "\n",
    "    train_cfg, model_cfg = load_configs()\n",
    "\n",
    "    updated_train_cfg = deepcopy(train_cfg)\n",
    "    updated_model_cfg = deepcopy(model_cfg)\n",
    "\n",
    "    if training_overrides:\n",
    "        _deep_update(updated_train_cfg['training']['recognition'], training_overrides)\n",
    "    if model_overrides:\n",
    "        _deep_update(updated_model_cfg['sign_recognition'], model_overrides)\n",
    "\n",
    "    with TRAIN_CONFIG_PATH.open('w') as fp:\n",
    "        yaml.safe_dump(updated_train_cfg, fp, sort_keys=False)\n",
    "    with MODEL_CONFIG_PATH.open('w') as fp:\n",
    "        yaml.safe_dump(updated_model_cfg, fp, sort_keys=False)\n",
    "\n",
    "    try:\n",
    "        yield updated_train_cfg, updated_model_cfg\n",
    "    finally:\n",
    "        with TRAIN_CONFIG_PATH.open('w') as fp:\n",
    "            yaml.safe_dump(train_cfg, fp, sort_keys=False)\n",
    "        with MODEL_CONFIG_PATH.open('w') as fp:\n",
    "            yaml.safe_dump(model_cfg, fp, sort_keys=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e135859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_recognition import train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d2964",
   "metadata": {},
   "source": [
    "## Run experiments\n",
    "\n",
    "Execute each cell below to run the configured experiments. Results are stored in the `EXPERIMENTS` list and later summarised.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "experiment_outputs = []\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    print(f\"\n",
    "=== Running: {exp.name} ===\")\n",
    "    print(exp.notes)\n",
    "    with apply_experiment_overrides(exp.training_overrides, exp.model_overrides):\n",
    "        result = train()\n",
    "    exp.result = result\n",
    "    experiment_outputs.append({**asdict(exp), 'result': result})\n",
    "    print(\"Top-1 accuracy:\", result['accuracy'])\n",
    "    print(\"Top-5 accuracy:\", result['top5'])\n",
    "    print(\"Checkpoint:\", result['checkpoint_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c678d",
   "metadata": {},
   "source": [
    "## Experiment summary table\n",
    "\n",
    "The table below aggregates the outcomes of every experiment, highlighting the best accuracy and the corresponding checkpoint path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def summarise_experiments(experiments):\n",
    "    rows = []\n",
    "    for exp in experiments:\n",
    "        if not exp.result:\n",
    "            continue\n",
    "        rows.append(\n",
    "            {\n",
    "                'Experiment': exp.name,\n",
    "                'Top-1 Accuracy': exp.result['accuracy'],\n",
    "                'Top-5 Accuracy': exp.result['top5'],\n",
    "                'Checkpoint': exp.result['checkpoint_path'],\n",
    "                'Notes': exp.notes,\n",
    "            }\n",
    "        )\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values('Top-1 Accuracy', ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "summary_df = summarise_experiments(EXPERIMENTS)\n",
    "if not summary_df.empty:\n",
    "    display(summary_df)\n",
    "    best_row = summary_df.iloc[0]\n",
    "    print(\"\n",
    "BEST MODEL SUMMARY\")\n",
    "    print(\"Experiment:\", best_row['Experiment'])\n",
    "    print(\"Top-1 Accuracy:\", best_row['Top-1 Accuracy'])\n",
    "    print(\"Top-5 Accuracy:\", best_row['Top-5 Accuracy'])\n",
    "    print(\"Checkpoint Path:\", best_row['Checkpoint'])\n",
    "else:\n",
    "    print(\"No experiments have been executed yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7700f7",
   "metadata": {},
   "source": [
    "## Inspect latest training logs\n",
    "\n",
    "Review the training log tail to demonstrate convergence behaviour during the presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = PROJECT_ROOT / 'logs'\n",
    "if log_dir.exists():\n",
    "    log_files = list(log_dir.glob('*.log'))\n",
    "    if log_files:\n",
    "        latest_log = max(log_files, key=lambda p: p.stat().st_mtime)\n",
    "        print(f\"Latest log file: {latest_log}\")\n",
    "        with latest_log.open() as fp:\n",
    "            lines = fp.readlines()\n",
    "        print(''.join(lines[-50:]))\n",
    "    else:\n",
    "        print('No log files found in the logs directory yet.')\n",
    "else:\n",
    "    print('Log directory not found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba5310",
   "metadata": {},
   "source": [
    "## Presentation talking points\n",
    "\n",
    "- Highlight the steady improvement from the baseline to the fine-tuned models (Top-1 and Top-5 accuracy trends).\n",
    "- Emphasise that checkpoints are stored in `models/checkpoints/sign_recognition/` and list the chosen best model path.\n",
    "- Discuss next planned experiments (label smoothing, SWA, alternative backbones).\n",
    "- Reiterate dataset challenges (54 classes, ~5 videos each) and how augmentation + transfer learning mitigated them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
